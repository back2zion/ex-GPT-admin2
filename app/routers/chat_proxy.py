"""
Chat Proxy Router - layout.htmlì˜ /api/chat_stream ìš”ì²­ì„ ds-apië¡œ í”„ë¡ì‹œ
"""
from fastapi import APIRouter, Request, HTTPException, Depends
from fastapi.responses import StreamingResponse
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import httpx
import json
import asyncio
from datetime import datetime

from app.models import UsageHistory
from app.core.database import get_db
from app.core.config import settings

router = APIRouter(prefix="/api", tags=["chat-proxy"])

# vLLM API URL (Docker ì»¨í…Œì´ë„ˆì—ì„œ host ì ‘ê·¼)
import os
LLM_API_URL = os.getenv("LLM_API_URL", "http://host.docker.internal:8000")


class ChatStreamRequest(BaseModel):
    """layout.htmlì—ì„œ ë³´ë‚´ëŠ” ì±„íŒ… ìš”ì²­"""
    message: str
    session_id: Optional[str] = None  # Optionalë¡œ ë³€ê²½
    user_id: str = "anonymous"
    think_mode: bool = False
    file_ids: List[str] = []
    history: List[Dict[str, Any]] = []


async def save_usage_to_db(
    db: AsyncSession,
    user_id: str,
    session_id: str,
    question: str,
    answer: str,
    conversation_title: Optional[str] = None
):
    """usage_historyì— ëŒ€í™” ì €ì¥"""
    try:
        # í•´ë‹¹ ì„¸ì…˜ì˜ ì²« ëŒ€í™”ì¸ì§€ í™•ì¸
        if not conversation_title:
            query = select(UsageHistory).filter(
                UsageHistory.session_id == session_id
            ).limit(1)
            result = await db.execute(query)
            existing = result.scalar_one_or_none()

            # ì²« ëŒ€í™”ë¼ë©´ ì§ˆë¬¸ìœ¼ë¡œ ì œëª© ìƒì„±
            if not existing:
                conversation_title = question[:50] + "..." if len(question) > 50 else question

        # ìƒˆ ë ˆì½”ë“œ ìƒì„±
        usage_record = UsageHistory(
            user_id=user_id,
            session_id=session_id,
            conversation_title=conversation_title,
            question=question,
            answer=answer,
            model_name="ex-GPT",
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )

        db.add(usage_record)
        await db.commit()

        print(f"âœ… Usage saved to DB: user_id={user_id}, session_id={session_id}")

    except Exception as e:
        print(f"âŒ Failed to save usage to DB: {e}")
        await db.rollback()


@router.post("/chat_stream")
async def chat_stream_proxy(
    request: ChatStreamRequest,
    db: AsyncSession = Depends(get_db),
    raw_request: Request = None
):
    """
    layout.htmlì˜ ì±„íŒ… ìš”ì²­ì„ ds-apië¡œ í”„ë¡ì‹œí•˜ê³  ì‘ë‹µì„ ìŠ¤íŠ¸ë¦¬ë°
    ë™ì‹œì— usage_historyì— ì €ì¥
    """

    # session_idê°€ ì—†ìœ¼ë©´ ìë™ ìƒì„± (user_id + timestamp)
    if not request.session_id:
        import time
        request.session_id = f"{request.user_id}_session_{int(time.time())}"

    # vLLM OpenAI í˜¸í™˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    messages = []

    # historyê°€ ìˆìœ¼ë©´ ì¶”ê°€
    if request.history:
        for msg in request.history:
            if isinstance(msg, dict):
                messages.append({"role": msg.get("role", "user"), "content": msg.get("content", "")})

    # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
    messages.append({"role": "user", "content": request.message})

    llm_payload = {
        "model": "default-model",
        "messages": messages,
        "stream": True,
        "max_tokens": 2000,
        "temperature": 0.7
    }

    # ì‘ë‹µ ë°ì´í„° ëˆ„ì  (DB ì €ì¥ìš©)
    accumulated_response = ""

    async def stream_and_save():
        nonlocal accumulated_response

        try:
            # LLM APIë¡œ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ (follow_redirects=True ì¶”ê°€)
            async with httpx.AsyncClient(timeout=120.0, follow_redirects=True) as client:
                async with client.stream(
                    "POST",
                    f"{LLM_API_URL}/v1/chat/completions",
                    json=llm_payload,
                    headers={"Content-Type": "application/json"}
                ) as response:

                    if response.status_code != 200:
                        error_msg = f"LLM API ì˜¤ë¥˜: {response.status_code}"
                        yield f"data: {json.dumps({'content': error_msg}, ensure_ascii=False)}\n\n"
                        yield "data: [DONE]\n\n"

                        # ì˜¤ë¥˜ë„ DBì— ì €ì¥
                        await save_usage_to_db(
                            db=db,
                            user_id=request.user_id,
                            session_id=request.session_id,
                            question=request.message,
                            answer=error_msg
                        )
                        return

                    # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì „ë‹¬ ë° íŒŒì‹± (vLLM OpenAI í˜¸í™˜ í˜•ì‹)
                    async for line in response.aiter_lines():
                        if line:
                            # ì‘ë‹µ ë°ì´í„° íŒŒì‹± ë° ëˆ„ì 
                            if line.startswith("data: "):
                                data_str = line[6:].strip()
                                if data_str == "[DONE]":
                                    yield "data: [DONE]\n\n"
                                    break

                                try:
                                    data = json.loads(data_str)

                                    # vLLM í˜•ì‹: choices[0].delta.content ì¶”ì¶œ
                                    if "choices" in data and len(data["choices"]) > 0:
                                        delta = data["choices"][0].get("delta", {})
                                        token = delta.get("content", "")

                                        if token:
                                            accumulated_response += token
                                            # layout.htmlì´ ê¸°ëŒ€í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (type: "token" í•„ë“œ ì¶”ê°€)
                                            yield f"data: {json.dumps({'type': 'token', 'content': token}, ensure_ascii=False)}\n\n"

                                except json.JSONDecodeError:
                                    pass

            # ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ í›„ DBì— ì €ì¥
            # thinking ë‚´ìš©ë§Œ ìˆëŠ” ê²½ìš°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
            if accumulated_response and not accumulated_response.strip().startswith('<think>'):
                await save_usage_to_db(
                    db=db,
                    user_id=request.user_id,
                    session_id=request.session_id,
                    question=request.message,
                    answer=accumulated_response
                )

        except Exception as e:
            error_msg = f"í”„ë¡ì‹œ ì˜¤ë¥˜: {str(e)}"
            print(f"âŒ Chat stream proxy error: {e}")
            yield f"data: {json.dumps({'content': error_msg}, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"

            # ì˜¤ë¥˜ë„ DBì— ì €ì¥
            await save_usage_to_db(
                db=db,
                user_id=request.user_id,
                session_id=request.session_id,
                question=request.message,
                answer=error_msg
            )

    return StreamingResponse(
        stream_and_save(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )


@router.get("/chat/sessions")
async def get_chat_sessions(
    user_id: Optional[str] = None,
    db: AsyncSession = Depends(get_db)
):
    """
    ì‚¬ìš©ìì˜ ëŒ€í™” ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ (ì‚¬ì´ë“œë°”ìš©)

    Returns:
        List of unique sessions with their titles and latest message time
    """
    from sqlalchemy import func, distinct

    # ê° ì„¸ì…˜ì˜ ì²« ë©”ì‹œì§€(ëŒ€í™” ì œëª©)ì™€ ìµœì‹  ì‹œê°„ ì¡°íšŒ
    query = select(
        UsageHistory.session_id,
        UsageHistory.conversation_title,
        func.max(UsageHistory.created_at).label('latest_time'),
        func.count(UsageHistory.id).label('message_count')
    ).group_by(
        UsageHistory.session_id,
        UsageHistory.conversation_title
    ).order_by(
        func.max(UsageHistory.created_at).desc()
    )

    if user_id:
        query = query.filter(UsageHistory.user_id == user_id)

    result = await db.execute(query)
    sessions = result.all()

    return {
        "sessions": [
            {
                "session_id": session.session_id,
                "title": session.conversation_title or "ëŒ€í™” ì œëª© ì—†ìŒ",
                "latest_time": session.latest_time.isoformat() if session.latest_time else None,
                "message_count": session.message_count
            }
            for session in sessions
        ]
    }


@router.get("/chat/sessions/{session_id}")
async def get_session_messages(
    session_id: str,
    db: AsyncSession = Depends(get_db)
):
    """
    íŠ¹ì • ì„¸ì…˜ì˜ ëª¨ë“  ë©”ì‹œì§€ ì¡°íšŒ
    """
    from sqlalchemy import asc

    query = select(UsageHistory).filter(
        UsageHistory.session_id == session_id
    ).order_by(asc(UsageHistory.created_at))

    result = await db.execute(query)
    messages = result.scalars().all()

    return {
        "session_id": session_id,
        "messages": [
            {
                "id": msg.id,
                "question": msg.question,
                "answer": msg.answer,
                "created_at": msg.created_at.isoformat() if msg.created_at else None
            }
            for msg in messages
        ]
    }


class STTConversationRequest(BaseModel):
    """STT ëŒ€í™” ì €ì¥ ìš”ì²­"""
    user_id: str
    session_id: str
    question: str  # "STT ì „ì‚¬í•´ì¤˜: filename.mp3"
    answer: str  # AI ë¶„ì„ íšŒì˜ë¡
    thinking_content: Optional[str] = None  # ìŒì„± ì „ì‚¬ ë‚´ìš©
    response_time: Optional[float] = None
    usage_metadata: Optional[Dict[str, Any]] = None


@router.post("/stt/save_conversation")
async def save_stt_conversation(
    request: STTConversationRequest,
    db: AsyncSession = Depends(get_db)
):
    """
    STT ëŒ€í™” ë‚´ì—­ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
    layout.htmlì˜ processVoiceToChat()ì—ì„œ í˜¸ì¶œ
    """
    try:
        # ëŒ€í™” ì œëª© ìƒì„± (ì²« ëŒ€í™”ì¸ ê²½ìš°)
        query = select(UsageHistory).filter(
            UsageHistory.session_id == request.session_id
        ).limit(1)
        result = await db.execute(query)
        existing = result.scalar_one_or_none()

        conversation_title = None
        if not existing:
            # STT ìš”ì²­ì„ì„ ëª…ì‹œí•œ ì œëª©
            conversation_title = f"ğŸ¤ {request.question[:50]}"

        # ìƒˆ ë ˆì½”ë“œ ìƒì„±
        usage_record = UsageHistory(
            user_id=request.user_id,
            session_id=request.session_id,
            conversation_title=conversation_title,
            question=request.question,
            answer=request.answer,
            thinking_content=request.thinking_content,
            response_time=request.response_time,
            model_name="ex-GPT-STT",  # STTì„ì„ ëª…ì‹œ
            usage_metadata=request.usage_metadata,
            created_at=datetime.utcnow(),
            updated_at=datetime.utcnow()
        )

        db.add(usage_record)
        await db.commit()
        await db.refresh(usage_record)

        print(f"âœ… STT conversation saved to DB: id={usage_record.id}, session_id={request.session_id}")

        return {
            "success": True,
            "id": usage_record.id,
            "message": "STT ëŒ€í™”ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤."
        }

    except Exception as e:
        print(f"âŒ Failed to save STT conversation to DB: {e}")
        await db.rollback()
        raise HTTPException(status_code=500, detail=f"STT ëŒ€í™” ì €ì¥ ì‹¤íŒ¨: {str(e)}")
