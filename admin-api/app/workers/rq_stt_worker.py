"""
RQ (Redis Queue) ê¸°ë°˜ STT ë³‘ë ¬ Worker
H100 2ëŒ€ë¥¼ í™œìš©í•œ ë³‘ë ¬ ì²˜ë¦¬

Architecture:
- GPU 0: Worker 1, 2 (2ê°œ í”„ë¡œì„¸ìŠ¤)
- GPU 1: Worker 3, 4 (2ê°œ í”„ë¡œì„¸ìŠ¤)
- ì´ 4ê°œ Worker ë™ì‹œ ì²˜ë¦¬

Usage:
    # Worker ì‹œìž‘ (í„°ë¯¸ë„ 1)
    rq worker stt-queue --with-scheduler

    # Worker ì‹œìž‘ (í„°ë¯¸ë„ 2)
    rq worker stt-queue --with-scheduler

    # Worker ì‹œìž‘ (í„°ë¯¸ë„ 3)
    rq worker stt-queue --with-scheduler

    # Worker ì‹œìž‘ (í„°ë¯¸ë„ 4)
    rq worker stt-queue --with-scheduler
"""
import os
import asyncio
from pathlib import Path
from typing import Optional
from redis import Redis
from rq import Queue, Worker
from rq.job import Job

# Redis ì—°ê²° (ê¸°ì¡´ admin-api Redis ìž¬ì‚¬ìš©)
redis_conn = Redis(
    host=os.getenv("REDIS_HOST", "localhost"),
    port=int(os.getenv("REDIS_PORT", "6379")),
    db=int(os.getenv("REDIS_DB", "0")),
    decode_responses=False  # bytes ëª¨ë“œ (ì„±ëŠ¥)
)

# STT ìž‘ì—… í
stt_queue = Queue("stt-queue", connection=redis_conn)


def process_audio_file_rq(
    batch_id: int,
    audio_file_path: str,
    gpu_id: Optional[int] = None
) -> dict:
    """
    RQ Job: ë‹¨ì¼ ì˜¤ë””ì˜¤ íŒŒì¼ STT ì²˜ë¦¬

    Args:
        batch_id: ë°°ì¹˜ ID
        audio_file_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        gpu_id: GPU ID (0 ë˜ëŠ” 1)

    Returns:
        dict: ì²˜ë¦¬ ê²°ê³¼ {"success": bool, "txt_file": str}

    ì´ í•¨ìˆ˜ëŠ” RQ Workerì—ì„œ ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
    """
    # GPU ì„¤ì •
    if gpu_id is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        print(f"ðŸŽ® Using GPU {gpu_id}")

    # ë¹„ë™ê¸° í•¨ìˆ˜ë¥¼ ë™ê¸° ë°©ì‹ìœ¼ë¡œ ì‹¤í–‰
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        result = loop.run_until_complete(
            _process_audio_async(batch_id, audio_file_path)
        )
        return result
    finally:
        loop.close()


async def _process_audio_async(batch_id: int, audio_file_path: str) -> dict:
    """
    ë¹„ë™ê¸° STT ì²˜ë¦¬ ë¡œì§ (ë‚´ë¶€ í•¨ìˆ˜)
    """
    from app.services.stt_client_service import STTClientService
    from app.core.database import get_async_session
    from app.models.stt import STTTranscription
    from datetime import datetime
    import httpx

    print(f"ðŸŽ¤ Processing: {audio_file_path}")

    stt_client = STTClientService(api_base_url="http://localhost:9200")
    filename = Path(audio_file_path).stem
    task_id = None
    transcription_text = ""
    txt_file = None
    status = "failed"
    error_message = None
    processing_started_at = datetime.utcnow()
    processing_completed_at = None

    try:
        # 1. ex-GPT-STTì— ì œì¶œ
        print(f"ðŸ“¤ Submitting audio file to ex-GPT-STT: {audio_file_path}")
        submit_result = await stt_client.submit_audio(
            audio_file_path=audio_file_path,
            meeting_title=filename,
            sender_name="RQ Worker"
        )
        task_id = submit_result.get("task_id")
        if not task_id:
            raise ValueError("No task_id returned from STT API after submission.")
        print(f"âœ… Audio submitted. Task ID: {task_id}")

        # 2. ì™„ë£Œ ëŒ€ê¸°
        print(f"â³ Waiting for task {task_id} completion...")
        task_result = await stt_client.wait_for_completion(
            task_id=task_id,
            max_wait_time=1800  # 30ë¶„
        )
        print(f"âœ… Task {task_id} completed. Status: {task_result.get('status')}")

        # 3. txt íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë° ì €ìž¥
        transcription_text = await stt_client.download_transcription_file(task_id)
        output_dir = Path("/data/stt-results") / f"batch_{batch_id}"
        output_dir.mkdir(parents=True, exist_ok=True)
        txt_file = output_dir / f"{filename}.txt"
        txt_file.write_text(transcription_text, encoding="utf-8")
        print(f"âœ… Transcription saved to: {txt_file}")

        status = "success"
        processing_completed_at = datetime.utcnow()

    except httpx.HTTPStatusError as e:
        error_message = f"HTTP Status Error from STT API: {e.response.status_code} - {e.response.text}"
        print(f"âŒ {error_message}")
    except httpx.RequestError as e:
        error_message = f"Network/Request Error connecting to STT API: {e}"
        print(f"âŒ {error_message}")
    except asyncio.TimeoutError:
        error_message = f"STT task {task_id} timed out after 1800 seconds."
        print(f"âŒ {error_message}")
    except ValueError as e:
        error_message = f"Configuration/Value Error: {e}"
        print(f"âŒ {error_message}")
    except Exception as e:
        error_message = f"An unexpected error occurred during STT processing: {e}"
        print(f"âŒ {error_message}")
    finally:
        processing_completed_at = processing_completed_at or datetime.utcnow()
        # 4. DB ì €ìž¥
        async for db in get_async_session():
            transcription = STTTranscription(
                batch_id=batch_id,
                audio_file_path=audio_file_path,
                transcription_text=transcription_text,
                status=status,
                ex_gpt_task_id=task_id,
                processing_started_at=processing_started_at,
                processing_completed_at=processing_completed_at,
                error_message=error_message
            )
            db.add(transcription)
            await db.commit()
            break

    return {
        "success": status == "success",
        "txt_file": str(txt_file) if txt_file else None,
        "task_id": task_id,
        "status": status,
        "error_message": error_message
    }


def enqueue_batch_processing(batch_id: int, audio_files: list) -> list:
    """
    ë°°ì¹˜ ì²˜ë¦¬ ìž‘ì—…ì„ RQ íì— ë“±ë¡ (GPU ë¶„ì‚°)

    Args:
        batch_id: ë°°ì¹˜ ID
        audio_files: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ëª©ë¡

    Returns:
        list: RQ Job ID ëª©ë¡
    """
    jobs = []

    # H100 2ëŒ€ì— ë¶„ì‚° (ë¼ìš´ë“œ ë¡œë¹ˆ)
    for idx, audio_file in enumerate(audio_files):
        gpu_id = idx % 2  # GPU 0, 1 ë²ˆê°ˆì•„ ì‚¬ìš©

        job = stt_queue.enqueue(
            process_audio_file_rq,
            batch_id=batch_id,
            audio_file_path=audio_file,
            gpu_id=gpu_id,
            job_timeout="2h",  # ìµœëŒ€ 2ì‹œê°„
            result_ttl=86400,  # ê²°ê³¼ ë³´ê´€ 24ì‹œê°„
            failure_ttl=604800,  # ì‹¤íŒ¨ ë¡œê·¸ ë³´ê´€ 7ì¼
            meta={
                "batch_id": batch_id,
                "filename": Path(audio_file).name,
                "gpu_id": gpu_id
            }
        )

        jobs.append(job.id)
        print(f"ðŸ“¤ Enqueued: {Path(audio_file).name} â†’ GPU {gpu_id} (Job: {job.id[:8]})")

    return jobs


def get_batch_progress(batch_id: int) -> dict:
    """
    ë°°ì¹˜ ì§„í–‰ ìƒí™© ì¡°íšŒ (RQ ê¸°ë°˜)

    Args:
        batch_id: ë°°ì¹˜ ID

    Returns:
        dict: {
            "total": int,
            "queued": int,
            "started": int,
            "finished": int,
            "failed": int
        }
    """
    # íì—ì„œ ëª¨ë“  ìž‘ì—… ì¡°íšŒ
    registry = stt_queue.started_job_registry
    failed_registry = stt_queue.failed_job_registry
    finished_registry = stt_queue.finished_job_registry

    # ë°°ì¹˜ ê´€ë ¨ ìž‘ì—…ë§Œ í•„í„°ë§
    queued_jobs = [j for j in stt_queue.jobs if j.meta.get("batch_id") == batch_id]
    started_jobs = [j for j in registry.get_job_ids() if Job.fetch(j, connection=redis_conn).meta.get("batch_id") == batch_id]
    finished_jobs = [j for j in finished_registry.get_job_ids() if Job.fetch(j, connection=redis_conn).meta.get("batch_id") == batch_id]
    failed_jobs = [j for j in failed_registry.get_job_ids() if Job.fetch(j, connection=redis_conn).meta.get("batch_id") == batch_id]

    total = len(queued_jobs) + len(started_jobs) + len(finished_jobs) + len(failed_jobs)

    return {
        "total": total,
        "queued": len(queued_jobs),
        "started": len(started_jobs),
        "finished": len(finished_jobs),
        "failed": len(failed_jobs),
        "progress_percentage": (len(finished_jobs) / total * 100) if total > 0 else 0
    }


def cancel_batch(batch_id: int) -> int:
    """
    ë°°ì¹˜ ìž‘ì—… ì·¨ì†Œ

    Args:
        batch_id: ë°°ì¹˜ ID

    Returns:
        int: ì·¨ì†Œëœ ìž‘ì—… ìˆ˜
    """
    cancelled_count = 0

    # ëŒ€ê¸° ì¤‘ì¸ ìž‘ì—… ì·¨ì†Œ
    for job in stt_queue.jobs:
        if job.meta.get("batch_id") == batch_id:
            job.cancel()
            job.delete()
            cancelled_count += 1

    print(f"ðŸš« Cancelled {cancelled_count} jobs for batch {batch_id}")
    return cancelled_count


# Worker ì‹œìž‘ ìŠ¤í¬ë¦½íŠ¸ (ë³„ë„ íŒŒì¼ë¡œ ì‹¤í–‰)
if __name__ == "__main__":
    """
    Usage:
        python -m app.workers.rq_stt_worker

    ë˜ëŠ” rq CLI:
        rq worker stt-queue --url redis://localhost:6379/0
    """
    import sys

    # GPU IDë¥¼ ëª…ë ¹ì¤„ ì¸ìžë¡œ ë°›ê¸°
    gpu_id = int(sys.argv[1]) if len(sys.argv) > 1 else None

    if gpu_id is not None:
        os.environ["CUDA_VISIBLE_DEVICES"] = str(gpu_id)
        print(f"ðŸŽ® Worker assigned to GPU {gpu_id}")

    # RQ Worker ì‹œìž‘
    worker = Worker([stt_queue], connection=redis_conn)
    worker.work(with_scheduler=True)
